# main.py
import os
from langchain.agents import create_agent
from langchain.tools import tool
from langchain_community.document_loaders import TextLoader # type: ignore
from langchain_chroma import Chroma # type: ignore
from langchain_huggingface import HuggingFaceEmbeddings # type: ignore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI # type: ignore
from dotenv import load_dotenv # type: ignore

load_dotenv()

# === 1. API –∫–ª—é—á ===
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
if not OPENROUTER_API_KEY:
    raise EnvironmentError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ OPENROUTER_API_KEY –≤ —Ñ–∞–π–ª–µ .env")

# === 2. LLM (—Ä–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ OpenRouter) ===
# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å: gemini-flash-1.5
llm = ChatOpenAI(
    model="google/gemini-2.5-flash",
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
    temperature=0.4
)

# === 3. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –∫–Ω–∏–≥–∏ ===
BOOK_PATH = "data/book.txt"
if not os.path.exists(BOOK_PATH):
    raise FileNotFoundError(f"–§–∞–π–ª –∫–Ω–∏–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {BOOK_PATH}")

loader = TextLoader(BOOK_PATH, encoding="utf-8")
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=350,
    separators=["\n\n", "\n", ". ", "! ", "? ", " ‚Äî ", " ", ""]
)
chunks = text_splitter.split_documents(docs)

# === 4. –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (Chroma) ===
CHROMA_PATH = "./chroma_godfather"

embedding = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

if os.path.exists(CHROMA_PATH):
    vectorstore = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding)
else:
    vectorstore = Chroma.from_documents(
        chunks, embedding, persist_directory=CHROMA_PATH
    )
    print("‚úÖ Chroma –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω.")

retriever = vectorstore.as_retriever(search_kwargs={"k": 15})

# === 5. Tool: answer_from_book ===
@tool
def answer_from_book(query: str) -> str:
    """Answer a question about 'The Godfather' using retrieval from the English book. Respond in Russian."""
    
    # üîç –®–∞–≥ 2: –ò—Å–∫–∞—Ç—å –ø–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º—É –∑–∞–ø—Ä–æ—Å—É
    retrieved_docs = retriever.invoke(query)
    context = "\n\n".join(doc.page_content for doc in retrieved_docs)

    # üß† –®–∞–≥ 3: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º
    synthesis_llm = ChatOpenAI(
        model="google/gemini-2.5-flash",
        base_url="https://openrouter.ai/api/v1",
        api_key=OPENROUTER_API_KEY,
        temperature=0.4
    )

    prompt = (
        "Answer the following question based ONLY on the provided context from the novel 'The Godfather' by Mario Puzo.\n"
        "The context is in English. You MUST respond in Russian.\n"
        "If the context does not contain enough information, respond exactly: \"–í –∫–Ω–∏–≥–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É.\"\n\n"
        f"Question: {query}\n\nContext:\n{context}\n\nAnswer in Russian:"
    )
    response = synthesis_llm.invoke([{"role": "user", "content": prompt}])
    return response.content.strip()

# === 6. –ê–≥–µ–Ω—Ç —á–µ—Ä–µ–∑ create_agent() ===
# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç ‚Äî —Å—Ç—Ä–æ–≥–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º
SYSTEM_PROMPT = (
    "You are a helpful AI assistant specialized in answering questions about the novel 'The Godfather' by Mario Puzo. "
    "You have access to a tool that retrieves relevant passages from the book. "
    "ALWAYS use the tool to answer questions ‚Äî never rely on prior knowledge. "
    "The tool returns context in English. You MUST respond to the user in Russian. "
    "If the tool returns insufficient information, say exactly: \"–í –∫–Ω–∏–≥–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É.\""
)

agent = create_agent(
    model=llm,
    tools=[answer_from_book],
    system_prompt=SYSTEM_PROMPT
)

# === 7. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ü–∏–∫–ª ===
if __name__ == "__main__":
    print("ü§ñ RAG-–∞–≥–µ–Ω—Ç –ø–æ –∫–Ω–∏–≥–µ ¬´–ö—Ä—ë—Å—Ç–Ω—ã–π –æ—Ç–µ—Ü¬ª –∑–∞–ø—É—â–µ–Ω!")
    print("üí¨ –ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.")
    print("üö™ –ß—Ç–æ–±—ã –≤—ã–π—Ç–∏, –≤–≤–µ–¥–∏—Ç–µ: exit, quit –∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C\n")

    try:
        while True:
            question = input("–í–æ–ø—Ä–æ—Å: ").strip()
            if not question:
                continue
            if question.lower() in ("exit", "quit", "–≤—ã—Ö–æ–¥"):
                print("–ü–æ–∫–∞! üëã")
                break

            result = agent.invoke({"messages": [HumanMessage(content=question)]})
            answer = result["messages"][-1].content
            print(f"–û—Ç–≤–µ—Ç: {answer}\n")

    except KeyboardInterrupt:
        print("\n\n–ü–æ–∫–∞! üëã")